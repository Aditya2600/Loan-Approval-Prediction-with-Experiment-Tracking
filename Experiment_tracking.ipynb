{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE__2Q3YdDP3"
      },
      "source": [
        "# Experiment tracking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWEqAAvOdDQD"
      },
      "source": [
        "The usual process for building an EndToEnd machine learning project involves collecting and processing raw data, analyzing it features at steps, training different algorithms, evaluating them, and deploying the best model on some platform for user access.\n",
        "It seems fairly straightforward, right? But in reality it is not. There are several complexities that arise along the way. Due to the circular nature of this process, its more about experimenting and trying out different things that may work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UN9LmIRVdDQG"
      },
      "source": [
        "* ML is not just code. It is code plus data that we need to keep a track of. Data can be sourced from multiple storage units\n",
        "* use different models and model hyperparameters\n",
        "* run the same code in a different environment\n",
        "* Model governance is another important aspect, where everything starting from experimentation to deployment is tracked for auditing purposes, where models are tested for speed, accuracy, drift while in production to avoid inaccuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30ZP_kYjdDQH"
      },
      "source": [
        "You can think of experiments as the process of building an ML model. When we say experiment run, we mean each trial in an ML experiment. So the ML experiment is actually the whole process that a data scientist may start playing with some data, models and hyperparameters. Each of these trials is an experiment run.\n",
        "\n",
        "Experiment tracking is the process of keeping track of all the relevant information from ML experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shaRpUiWdDQH"
      },
      "source": [
        "* **Organize** all the necessary components of a specific experiment. It's important to have everything in one place and know where it is so you can use them later.\n",
        "* **Reproduce** past results (easily) using saved experiments.\n",
        "* **Log** iterative improvements across time, data, ideas, teams, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMEvyqIjdDQI"
      },
      "source": [
        "If you are working in a finance company and are tasked with creating a ml model that based on certain conditions classify if the applicant should be given a loan or not"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6EXUE2KMdDQK"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'dsml_env (Python 3.9.21)' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'conda install -n dsml_env ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "rXucHftgdDQO",
        "outputId": "c194e7cb-e609-4a40-eb17-2b5a521563d3"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('data.csv')\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evt9ziMSdDQR",
        "outputId": "80c70ba4-5eb5-45cf-e786-09df601c2547"
      },
      "outputs": [],
      "source": [
        "train_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1v_xFjBudDQS"
      },
      "source": [
        "## Binary Encoding of Categorical Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sDVgn_sSdDQT"
      },
      "outputs": [],
      "source": [
        "train_df['Gender']= train_df['Gender'].map({'Male':0, 'Female':1})\n",
        "train_df['Married']= train_df['Married'].map({'No':0, 'Yes':1})\n",
        "train_df['Loan_Status']= train_df['Loan_Status'].map({'N':0, 'Y':1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQaEBh66dDQU",
        "outputId": "8418327b-2513-4493-eab6-e201e5292804"
      },
      "outputs": [],
      "source": [
        "train_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_EU_BfWdDQV"
      },
      "source": [
        "## Checking for Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "qQZA5nlCdDQV",
        "outputId": "ed2bb386-5942-447e-f49e-cebe2d4cfde9"
      },
      "outputs": [],
      "source": [
        "train_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "2QwXQS-0dDQW",
        "outputId": "1afee3fb-8d3b-40ab-ecfe-923803b4f0fe"
      },
      "outputs": [],
      "source": [
        "## dropping all the missing values\n",
        "train_df = train_df.dropna()\n",
        "train_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU793swXdDQX"
      },
      "source": [
        "## Segregating the target variable from the features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6o2TDeZudDQY",
        "outputId": "8a1889bd-09e0-4490-99e2-766c80faf9b2"
      },
      "outputs": [],
      "source": [
        "X = train_df[['Gender', 'Married', 'ApplicantIncome', 'LoanAmount', 'Credit_History']]\n",
        "y = train_df.Loan_Status\n",
        "X.shape, y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p7A7N3bdDQa"
      },
      "source": [
        "## Splitting the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MqTDLZ_4dDQa"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBW-92nudDQc"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "W8z5Mvw7dDQf",
        "outputId": "bfa9a405-b68d-48d8-8877-412b9b54bff9"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier(max_depth=4, random_state=5)\n",
        "model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEcR-y6HdDQg"
      },
      "source": [
        "## Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSAg_2URdDQg",
        "outputId": "b0fa526b-c20a-4c5a-a75b-e3d1af4e868a"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "pred_val = model.predict(X_val)\n",
        "accuracy_score(y_val, pred_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxbBxJcvdDQh",
        "outputId": "0b60a124-87f0-49fd-9a8f-09cddafb29ed"
      },
      "outputs": [],
      "source": [
        "pred_train = model.predict(X_train)\n",
        "accuracy_score(y_train, pred_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq0kq5bydDQh"
      },
      "source": [
        "### We have created a model successfully, but now we have new set of data how do you proceed on working with it\n",
        "\n",
        "* we can change the data and run the code again\n",
        "    * but we will loose the output and results from the old data\n",
        "* we can create new cells below these to create a new model with the new data\n",
        "    * but then when we have a lot of experiments in one file it will be really difficult finding the one we want to look at\n",
        "* We can create new files for each experiment\n",
        "    * but for actually comparing the rsults and outputs you'll still have to ope each file and look into it closely"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB9sF4nVdDQh"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbveEzghdDQi"
      },
      "source": [
        "these are not the best ways of keeping track of the work and experiments that you perform, we need to create something that easy to manage, clearly shows the results and metrics, logs the changes and hyperparameters for us"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlJb1YqadDQi"
      },
      "source": [
        "# ML Flow\n",
        "\n",
        "https://mlflow.org/\n",
        "\n",
        "MLflow is an open-source platform to manage Machine Learning Lifecycle. In layman’s terms, it can track and store data, parameters, and metrics to be retrieved later or displayed nicely on a web interface.Furthermore, MLflow is a framework-agnostic tool, so any ML / DL framework can quickly adapt to the ecosystem that MLflow proposes.\n",
        "\n",
        "MLflow emerges as a platform that offers tools for tracking metrics, artifacts, and metadata."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHE_8nrbdDQj"
      },
      "source": [
        "## ML flow Tracking\n",
        "\n",
        "\n",
        "MLflow Tracking is an API-based tool for logging metrics, parameters, model versions, code versions, and files. MLflow Tracking is integrated with a UI for visualizing and managing artifacts, models, files, etc.\n",
        "\n",
        "Each MLflow Tracking session is organized and managed under the **concept of runs.**\n",
        "* A run refers to the execution of code where the artifact log is performed explicitly.\n",
        "\n",
        "By default, the runs are stored in the directory where the code session is executed. However, MLflow also allows storing artifacts on a local or remote server, for better collaboration.\n",
        "we'll st"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jlmV4TrdDQj"
      },
      "source": [
        "### getting started"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVoU_xeudDQj",
        "outputId": "75ca4c3a-3362-4321-a0e2-a3285fc499b2"
      },
      "outputs": [],
      "source": [
        "!pip3 install mlflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "R47FR_IUdDQk"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MinKZbfhdDQl"
      },
      "source": [
        "we'll start by setting up our experiment name under which we wanna perform all our work\n",
        "\n",
        "* An MLflow experiment is the primary unit of organization and access control for MLflow runs; all MLflow runs belong to an experiment. Experiment:{run,run.....run}\n",
        "* Experiments let you visualize, search for, and compare runs, as well as download run artifacts and metadata for analysis in other tools.\n",
        "* An MLflow run corresponds to a single execution of model code. Each run records the some information about that particulr trial:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zx8Y3nyXdDQm",
        "outputId": "fea95264-03a3-415e-abc3-7b6b70f23172"
      },
      "outputs": [],
      "source": [
        "mlflow.set_experiment(\"loan_status\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTodTzqZdDQn"
      },
      "source": [
        "Next, you can start to think about what do you want to keep track in your analysis/experiment. MLflow categorizes these into:\n",
        "\n",
        "* **Parameters** (via mlflow.log_param() ). Parameters are variables that you change or tweak when tuning your model.\n",
        "* **Metrics** (using mlflow.log_metric() ). Metrics are values that you want to measure as a result of tweaking your parameters. Typical metrics that are tracked can be items like F1 score, RMSE, MAE etc.\n",
        "* **Artifacts** (using mlflow.log_artifact() ). Artifacts are any other items that you wish to store. Typical artifacts to keep track of are PNGs of graphs,plots, confusion matrix, and also pickled model files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-C4HEswdDQq"
      },
      "source": [
        "Params are something you want to tune based on the metrics, whereas tags are some extra information that doesn't necessarily associate with the model's performance. there's no hard constraint on which to use to log which; they can be used interchangeably without error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvQ7JSuvdDQr",
        "outputId": "32f83f67-cc27-4619-b33b-030eb65d7974"
      },
      "outputs": [],
      "source": [
        "with mlflow.start_run():\n",
        "    model_rf = RandomForestClassifier(max_depth=4, random_state=5)\n",
        "    model_rf.fit(X_train, y_train)\n",
        "\n",
        "    pred_val = model_rf.predict(X_val)\n",
        "    val_acc=accuracy_score(y_val, pred_val)\n",
        "\n",
        "    pred_train = model_rf.predict(X_train)\n",
        "    train_acc=accuracy_score(y_train, pred_train)\n",
        "\n",
        "    mlflow.set_tag('mlflow.runName','first_run')\n",
        "    mlflow.log_param('max_depth',4)\n",
        "    mlflow.log_metric('val_acc',val_acc)\n",
        "    mlflow.log_metric('train_acc',train_acc)\n",
        "\n",
        "    mlflow.sklearn.log_model(model_rf, \"model\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3Zg06JhdDQr"
      },
      "source": [
        "we can use this with command to start the ml flow run and whatever we do inside of that start_run indent will be tracked\n",
        "\n",
        "inside that we create our first model and log the different parameters and metric for that model\n",
        "we set the name of the run and log the max depth of the rf model and also the acc score. All of the parameters and models are stored in files in the experiment folder with each runs having seperate folders. you can open those files to see the stored data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yuOU_8ZdDQt"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWxRE8jzdDQt"
      },
      "source": [
        "### mlflow ui"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCKvCCeTdDQu"
      },
      "source": [
        "MLflow also provides the option to view all the runs and experiments on a web based ui that is really easy to use and see the logged data.\n",
        "Launch the MLflow tracking UI for local viewing of run results.\n",
        "In the folder where you have this experiments run the command **mlflow ui**\n",
        "this will start an ml flow ui server that is by default open at port 5000 on your localhost or 127.0.0.1\n",
        "you can change the port by using -p port_num along with the command eg: **mlflow ui -p 8899**\n",
        "\n",
        "* open the correct link or copy the provided url from the command"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tyVhbWgdDQu"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR-TjangdDQu"
      },
      "source": [
        "here is the web ui launched on a browser, as you can see we are under the loan_status experimnet name and have a run that we created with the name first_run.\n",
        "there are several other informations as welllike the source code that we used, the user thatcreated that run and the model that we stored"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaxlR0SfdDQv"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB_VXvw2dDQv"
      },
      "source": [
        "if we click on any particular run we cansee more details about that run. we have here the all the detailsthat we logged for that particular model in a very easy to understand fashion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZy4hSaJdDQv"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmLY-wA8dDQv"
      },
      "source": [
        "Load the new data and proceed furthur"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8XF8sz9dDQw"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('data_new.csv')\n",
        "train_df.head()\n",
        "\n",
        "train_df['Gender']= train_df['Gender'].map({'Male':0, 'Female':1})\n",
        "train_df['Married']= train_df['Married'].map({'No':0, 'Yes':1})\n",
        "train_df['Loan_Status']= train_df['Loan_Status'].map({'N':0, 'Y':1})\n",
        "\n",
        "train_df = train_df.dropna()\n",
        "\n",
        "X = train_df[['Gender', 'Married', 'ApplicantIncome', 'LoanAmount', 'Credit_History']]\n",
        "y = train_df.Loan_Status\n",
        "X.shape, y.shape\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9FkFL7tdDQw"
      },
      "outputs": [],
      "source": [
        "with mlflow.start_run():\n",
        "    model_rf = RandomForestClassifier(max_depth=4, random_state=5)\n",
        "    model_rf.fit(X_train, y_train)\n",
        "\n",
        "    pred_val = model_rf.predict(X_val)\n",
        "    val_acc=accuracy_score(y_val, pred_val)\n",
        "\n",
        "    pred_train = model_rf.predict(X_train)\n",
        "    train_acc=accuracy_score(y_train, pred_train)\n",
        "\n",
        "    mlflow.set_tag('mlflow.runName','new_data')\n",
        "    mlflow.log_param('max_depth',4)\n",
        "    mlflow.log_metric('val_acc',val_acc)\n",
        "    mlflow.log_metric('train_acc',train_acc)\n",
        "    mlflow.set_tag('data file','data_new.csv')\n",
        "\n",
        "    mlflow.sklearn.log_model(model_rf, \"model\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgqs-P7PdDQx"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSC_T18VdDQx"
      },
      "source": [
        "if we go back to the web ui we can see that we have another run logged\n",
        "with the information we have we changed we added a new name and the name of the datafile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BazSA2bCdDQy"
      },
      "source": [
        "Now if we want to tune the RF model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RiYen9ddDQy"
      },
      "outputs": [],
      "source": [
        "\n",
        "def mlflow_runs(n_est,max_dep,i):\n",
        "    with mlflow.start_run():\n",
        "\n",
        "        model_rf = RandomForestClassifier(n_estimators=n_est, max_depth=max_dep, random_state=5)\n",
        "        model_rf.fit(X_train, y_train)\n",
        "\n",
        "        pred_val = model_rf.predict(X_val)\n",
        "        val_acc=accuracy_score(y_val, pred_val)\n",
        "\n",
        "        pred_train = model_rf.predict(X_train)\n",
        "        train_acc=accuracy_score(y_train, pred_train)\n",
        "\n",
        "        run=\"hyperparameter_run_\"+str(i)\n",
        "        mlflow.set_tag('mlflow.runName',run)\n",
        "        mlflow.log_param('n_estimators',n_est)\n",
        "        mlflow.log_param('max_depth',max_dep)\n",
        "        mlflow.log_metric('val_acc',val_acc)\n",
        "        mlflow.log_metric('train_acc',train_acc)\n",
        "        mlflow.set_tag('data file','data_new.csv')\n",
        "\n",
        "        mlflow.sklearn.log_model(model_rf, \"model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lblxp2QdDQy"
      },
      "outputs": [],
      "source": [
        "mlflow_runs(10,2,1)\n",
        "mlflow_runs(20,2,2)\n",
        "mlflow_runs(40,2,3)\n",
        "mlflow_runs(10,4,4)\n",
        "mlflow_runs(20,4,5)\n",
        "mlflow_runs(40,4,6)\n",
        "mlflow_runs(10,8,7)\n",
        "mlflow_runs(20,8,8)\n",
        "mlflow_runs(40,8,9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akZa2I-4dDQz"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjtLZACadDQz"
      },
      "source": [
        "here we can see there are 9 new runs that show how our model performed\n",
        "* we can see that increasing the number of tress improves the model a lot\n",
        "* if we have a deep model with less number of trees it seems to overfit because the train accuracy is very high but the valaccuracy is low\n",
        "you can also see there are two failed runs so they have no data associated with them\n",
        "\n",
        "now if you want to try out another model like knn for this task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pc_4vcAGdDQz",
        "outputId": "a6c58a70-42b8-41c8-bd97-ccde15a2daec"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "with mlflow.start_run():\n",
        "    knn_model= KNeighborsClassifier(n_neighbors=5)\n",
        "    knn_model.fit(X_train, y_train)\n",
        "\n",
        "    pred_val = knn_model.predict(X_val)\n",
        "    val_acc=accuracy_score(y_val, pred_val)\n",
        "\n",
        "    pred_train = knn_model.predict(X_train)\n",
        "    train_acc=accuracy_score(y_train, pred_train)\n",
        "\n",
        "    run=\"KNN\"\n",
        "    mlflow.set_tag('mlflow.runName',run)\n",
        "    mlflow.log_param('neighbors',5)\n",
        "    mlflow.log_metric('val_acc',val_acc)\n",
        "    mlflow.log_metric('train_acc',train_acc)\n",
        "    mlflow.set_tag('data file','data_new.csv')\n",
        "\n",
        "    cm=ConfusionMatrixDisplay.from_predictions( y_val,pred_val)\n",
        "    cm.figure_.savefig('confusion_mat.png')\n",
        "    mlflow.log_artifact('confusion_mat.png')\n",
        "\n",
        "    mlflow.sklearn.log_model(knn_model, \"model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bek79E8ddDQ0"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XPIvwixdDQ1"
      },
      "source": [
        "after all the testing and trying we can say that we will chose the random forest model with max depth =8 and number of trees=40"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dsml_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
